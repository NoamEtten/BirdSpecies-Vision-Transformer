{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize\n",
    "import PIL\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the feature extractor\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')\n",
    "\n",
    "# One-hot encode function\n",
    "def one_hot_encode(labels, num_classes):\n",
    "    return F.one_hot(labels, num_classes=num_classes)\n",
    "\n",
    "# Define the custom collate function\n",
    "def collate(batch):\n",
    "    images, labels = zip(*batch)\n",
    "\n",
    "    encoding = feature_extractor(images=list(images), return_tensors=\"pt\", do_rescale=False)\n",
    "    pixel_values = encoding[\"pixel_values\"]\n",
    "    labels = torch.tensor(labels)\n",
    "    return pixel_values, labels\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(), \n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.RandomPerspective(distortion_scale=0.2, p=0.5),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1), shear=10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 2.0)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Define data transformations for validation and testing (no augmentation)\n",
    "eval_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load the datasets with appropriate transformations\n",
    "train_dataset = datasets.ImageFolder(root='./train', transform=train_transform)\n",
    "valid_dataset = datasets.ImageFolder(root='./valid', transform=eval_transform)\n",
    "test_dataset = datasets.ImageFolder(root='./test', transform=eval_transform)\n",
    "\n",
    "# Select a subset of the training data for testing porpuses\n",
    "# subset_indices = np.random.choice(len(train_dataset), int(len(train_dataset) * 0.01), replace=False)  # for example, 10% of the data\n",
    "# subset_train_dataset = Subset(train_dataset, subset_indices)\n",
    "\n",
    "# Create DataLoader instances with the custom collate function\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, collate_fn=collate)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=4, collate_fn=collate)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, collate_fn=collate)\n",
    "\n",
    "for images, labels in DataLoader(train_dataset, batch_size=1, collate_fn=collate):\n",
    "    print(images.shape, labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(train_dataset.classes)  # Automatically determine the number of classes\n",
    "\n",
    "# Update the classifier to match the number of classes\n",
    "model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n",
    "model.classifier = nn.Linear(model.classifier.in_features, num_classes)\n",
    "model.config.num_labels = num_classes\n",
    "\n",
    "# Check if GPU is available and move model to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"full_model\"\n",
    "os.mkdir(f\"./Vit_saves/{model_id}\")\n",
    "\n",
    "results = {}\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
    "\n",
    "ending_epochs = 50\n",
    "starting_epoch = 0\n",
    "\n",
    "# Add an early stopping mechanism\n",
    "best_val_loss = float('inf')\n",
    "patience = 6\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(starting_epoch, ending_epochs, 1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    # Wrap your data loader with tqdm for a progress bar\n",
    "    # progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f'Epoch {epoch+1}/{ending_epochs}')\n",
    "    # for batch_idx, (pixel_values, labels) in progress_bar:\n",
    "    for batch_idx, (pixel_values, labels) in enumerate(train_loader):\n",
    "        pixel_values = pixel_values.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(pixel_values)\n",
    "        logits = outputs.logits\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        _, preds = torch.max(logits, 1)\n",
    "        num_corrects = torch.sum(preds == labels.data)\n",
    "        running_corrects += num_corrects\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        batch_accuracy = num_corrects.double() / labels.size(0)\n",
    "        running_accuracy = running_corrects / total_samples\n",
    "\n",
    "        # Update the progress bar with loss and accuracy\n",
    "        # progress_bar.set_postfix({'Running Loss': f'{running_loss/(batch_idx+1):.4f}', 'Running Accuracy': f'{running_accuracy:.4f}'})\n",
    "    \n",
    "    \n",
    "    model_save_path = f'./Vit_saves/{model_id}/model_epoch_{epoch+1}.pt'\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_running_corrects = 0\n",
    "    val_total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for val_pixel_values, val_labels in valid_loader:\n",
    "            val_pixel_values = val_pixel_values.to(device)\n",
    "            val_labels = val_labels.to(device)\n",
    "            \n",
    "            val_outputs = model(val_pixel_values)\n",
    "\n",
    "\n",
    "            val_logits = val_outputs.logits\n",
    "            val_loss = criterion(val_logits, val_labels)\n",
    "            _, val_preds = torch.max(val_logits, 1)\n",
    "            val_running_corrects += torch.sum(val_preds == val_labels.data)\n",
    "            val_total_samples += val_labels.size(0)\n",
    "\n",
    "\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    val_accuracy = val_running_corrects.double() / val_total_samples\n",
    "    print(f'Epoch {epoch+1}/{ending_epochs} done:', 'Running Loss: ', f'{running_loss/(batch_idx+1):.4f}', 'Running Accuracy: ', f'{running_accuracy:.4f}', f'Validation Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "    results[epoch+1] = [val_accuracy.item() , running_corrects.item() / total_samples]\n",
    "    \n",
    "    # Save the data to a file\n",
    "    with open(f'./Vit_saves/{model_id}/results.pickle', 'wb') as file:\n",
    "        pickle.dump(results, file)\n",
    "\n",
    "    if val_loss <= best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0  \n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    if patience_counter >= patience:\n",
    "        print(\"Finished due to early stopping\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data back from the file\n",
    "with open(f'./Vit_saves/{model_id}/results.pickle', 'rb') as file:\n",
    "    results = pickle.load(file)\n",
    "    # print(\"Loaded data:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the model with the highest validation accuracy\n",
    "best_epoch = max(results, key=lambda k: results[k][0])\n",
    "print(\"Best Epoch: \", best_epoch)\n",
    "\n",
    "model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n",
    "model.classifier = nn.Linear(model.classifier.in_features, num_classes)\n",
    "model.config.num_labels = num_classes\n",
    "model_path = f'./Vit_saves/{model_id}/model_epoch_{best_epoch}.pt'\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "# Check if GPU is available and move model to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = list(results.keys())\n",
    "val_accuracies = [result[0] for result in results.values()]\n",
    "train_accuracies = [result[1] for result in results.values()]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs, val_accuracies, label='Validation Accuracy', marker='o')\n",
    "plt.plot(epochs, train_accuracies, label='Training Accuracy', marker='o')\n",
    "plt.title('Training and Validation Accuracy Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xticks(epochs)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()  # Set the model to evaluation mode\n",
    "test_running_corrects = 0\n",
    "total_test_samples = 0\n",
    "\n",
    "with torch.no_grad():  # No gradients needed\n",
    "    for pixel_values, labels in test_loader:\n",
    "        pixel_values = pixel_values.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(pixel_values)\n",
    "        logits = outputs.logits\n",
    "        _, preds = torch.max(logits, 1)\n",
    "\n",
    "        test_running_corrects += torch.sum(preds == labels.data)\n",
    "        total_test_samples += labels.size(0)\n",
    "\n",
    "test_accuracy = test_running_corrects.double() / total_test_samples\n",
    "print(f'Test Accuracy: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Dictionaries to store correct predictions and total samples per class\n",
    "class_correct = dict()\n",
    "class_total = dict()\n",
    "incorrect_samples = []\n",
    "\n",
    "# Initialize dictionaries\n",
    "for class_index, class_name in enumerate(test_loader.dataset.classes):\n",
    "    class_correct[class_name] = 0\n",
    "    class_total[class_name] = 0\n",
    "\n",
    "with torch.no_grad():  # No gradients needed\n",
    "    for pixel_values, labels in test_loader:\n",
    "        pixel_values = pixel_values.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(pixel_values)\n",
    "        logits = outputs.logits\n",
    "        _, preds = torch.max(logits, 1)\n",
    "\n",
    "        # Compare predictions with the true labels\n",
    "        correct_tensor = preds.eq(labels.data.view_as(preds))\n",
    "        \n",
    "        # Iterate over correct tensor and count correct predictions\n",
    "        for i in range(len(labels)):\n",
    "            label = labels.data[i]\n",
    "            class_name = test_loader.dataset.classes[label]\n",
    "            class_correct[class_name] += correct_tensor[i].item()\n",
    "            class_total[class_name] += 1\n",
    "            if not correct_tensor[i].item():\n",
    "                incorrect_samples.append((pixel_values[i], label, preds[i].item()))\n",
    "\n",
    "# Calculate and print accuracy for each class\n",
    "for class_name in class_correct:\n",
    "    accuracy = 100 * float(class_correct[class_name]) / class_total[class_name]\n",
    "    if accuracy != 100:\n",
    "        print(f'Accuracy of {class_name} : {accuracy:.2f}%')\n",
    "\n",
    "print(\"Total number of incorrectly classified birds in the test dataset: \", len(incorrect_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = test_loader.dataset.classes\n",
    "\n",
    "def visualize_images(incorrect_images, class_names, grid_size=5):\n",
    "    \n",
    "    pixel_values = np.array([i[0].cpu() for i in incorrect_images])\n",
    "    labels = [i[1] for i in incorrect_images]\n",
    "    predicted_labels =  [i[2] for i in incorrect_images]\n",
    "    \n",
    "    # Calculate how many images to display\n",
    "    num_images = min(len(pixel_values), grid_size * grid_size)\n",
    "    \n",
    "    # Set up the figure size and grid layout\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    plt.suptitle('Missclassified Images', fontsize=16)\n",
    "\n",
    "    # Iterate over the images and labels up to num_images\n",
    "    for i in range(num_images):\n",
    "        img = pixel_values[i].cpu().numpy() # Convert tensor to numpy array\n",
    "        img = np.transpose(img, (1, 2, 0))  # Rearrange the dimensions\n",
    "        normalizer = Normalize(vmin=img.min(), vmax=img.max())\n",
    "        img = normalizer(img)\n",
    "        \n",
    "        # Determine subplot position\n",
    "        plt.subplot(grid_size, grid_size, i + 1)\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Label the image with true and predicted classes\n",
    "        true_class = class_names[labels[i]]\n",
    "        pred_class = class_names[predicted_labels[i]]\n",
    "        plt.title(f'True: {true_class}\\nPred: {pred_class}', fontsize=8, pad=3)\n",
    "\n",
    "    # Adjust layout to avoid overlap\n",
    "    plt.tight_layout(pad=1.0)\n",
    "    plt.show()\n",
    "\n",
    "visualize_images(incorrect_samples, class_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azureml_py38_PT_and_TF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
